# Equivalence procedure and Common Specification for software

Having discussed the classification and associated procedure for the initial approval, we will now detail the relevant requirements that software and manufacturers need to fulfil for products that are modified.

## Software modification and the equivalency procedure

As explained above, there is a distinction between the explicitly coded part of the algorithm and the resulting model. Also discussed before is the importance of software verification and validation, which is equally applicable to new/updated versions of software. Part of the QMS is aimed at handling modifications that aim to improve product quality. As the risk of modifications to a known quantity is generally lower compared to the risk of introducing a completely new product, the Regulation facilitates these modifications by reducing the requirements needed for approval of the new 'version' through the *equivalency procedure*.

Compared to regular software there is a component of AI models that can change without explicit human 'input', i.e. (re)training the model. This specific characteristic of AI software leads to the question whether training a model on new data and/or with adaptations to the model code encompasses a modification and whether this is a minor or major change.

A modification for a class IIb & III device requires a clinical evaluation unless the notified body is satisfied *'the device has been designed by modifying a device already marketed by the same manufacturer for the same intended purpose, provided that the manufacturer has demonstrated to the satisfaction of the notified body that the modifications do not adversely affect the benefit-risk ratio of the device*'.^[Art 54(2)(b) @REGULATIONEU2017a] A device is '*already marketed'* when it was approved under the Directive or the Regulation.^[@mdcgInterpretationArticle542020] For demonstrating equivalence, and thus be allowed to use existing clinical data for certification, the technical, biological and clinical characteristics of the devices are taken into account.^[Annex XIV, part A, 3 @REGULATIONEU2017a] As the biological aspects are not relevant and the clinical characteristics concern whether the device is used in the same clinical condition and purpose, the technical characteristics are important to consider for software.

Looking at the technical characteristics in the Regulation, the crux is in assessing whether the device is of '*similar design*', which specifically mentions 'software algorithms' as one of its properties.^[Annex XIV, part A, 3 @REGULATIONEU2017a] This includes both software (algorithms) driving a device or when it is intended to be used alone.^[@mdcgClinicalEvaluationEquivalence2020 6] The requirement of similar design is less strict for software that is not related to any medical purpose, for example the software responsible for the graphical user interface, which merely requires that it is justifiable that it does not negatively affect the usability, safety or clinical performance of the device.^[@mdcgClinicalEvaluationEquivalence2020 6] The guidance document specifically mentions that it is not required to demonstrate the equivalence of software code in case the software has been developed according to international standards for safe design and validation.^[@mdcgClinicalEvaluationEquivalence2020 6] Supporting this is that only a new Unique Device Identifier Production Identifier (UDI-PI), not a new Unique Device Identifier Device Identifier (UDI-DI), is required for minor software revisions. Major software changes require a new UDI-DI when the modifications affect the original performance, safety of the software or the interpretation of data.^[Annex VI Part C, 6.5.2 @REGULATIONEU2017a] These modifications include new or modified algorithms, database structures, operating platform, architecture or new user interfaces or new channels for interoperability.^[Annex VI part C, 6.5.3 @REGULATIONEU2017a]

Looking at machine learning models, modifying the software code describing the model itself is clearly covered under the Regulation similar to conventional software. Albeit that as small changes in the training algorithm could have substantial hard to predict consequences, it is prudent that these kind of changes require additional clinical evaluation unless their effect is justifiably small. The aforementioned does raise the question whether (re)training of the model modifies the algorithm or whether this is merely a minor software revision.

Specifically looking at the wording of 'software algorithm', retraining does not seem to fall under that definition given that the software code is equivalent in such cases. An argument against that is that the (re)trained model that is used for inference is likely different from the model that was originally trained. Regardless of whether it should be considered as the same device or as a modified device, the crux here is again the validation and verification of the performance and robustness of this software. In (re)training using approximately equivalent data would detrimentally affect performance, this raises the questions whether the model was sufficiently robust to begin with.

It is technically possible to automate parts of the verification software, which facilitates a bigger role for post-surveillance checks on data by ongoing verification of the performance of the algorithm. Although there is no reason to cut AI more slack in pre-approval evidence generation. If retraining is part of the plans for a device, manufacturers should make the necessary adaptations to their quality management system to ensure that this is possible without negatively impacting performance and with sufficient guarantees concerning robustness.

There are opportunities for data generation and verification post-market by using the technical documentation and imploring the sharing of data as a regulatory requirement. The Medical Device Regulation encourages the establishment of clinical data registries,[Art 108 @REGULATIONEU2017a] which considering the importance of data for software and more specifically training AI models, offers further opportunity to use this data for improving medical devices. Using data from similar devices is possible even when it is not sufficient to demonstrate equivalence. It can then be used for ensuring the risk management system, the scope of the clinical evaluation through identification of special performance or safety concerns, provide information for the post-market surveillance system, identifying relevant clinical outcome parameters and minimal requirements for assessing a clinically relevant effect.^[@mdcgClinicalEvaluationEquivalence2020 14]

Evidently, there is a balance to be struck between the free sharing of data and the commercial interest of manufacturers in that data. Also, setting up registries does require an investment, which may not materialise when the profitable to the investor inn setting up the registry is uncertain. Given that the EUDAMED database is yet to be implemented with the first module on actor registration planned for December 2021, it is clear that there are still technical barriers to the creation of a system where these heterogeneous data are easily shared.

## A Common Specification or future amendments for AI?

There seems to be a general anticipation that the development of AI and machine learning will have major implications for regulatory frameworks. Specifically for medical devices, it has been stated that further amendments to the regulatory framework and guidelines for medical devices are required in the near future in light of advances in AI and machine learning.^[@minssenWhenDoesStandAlone2020a] While we concur that more regulatory clarity concerning these technologies is warranted, it is unclear what the specific advances are that required addressing in regulatory amendments. Therefore, we present our view based on the perspective on AI as software that we have taken in this article.

In our analysis we have seen that the crux in regulating software in medical devices is the validation aspect. The question is whether the degree to which AI models can be validated is compliant with the regulatory requirements concerning robustness and efficacy in line with the benefit-risk of such software in medical devices. As such, there is a need for further technical guidelines. Whereas the necessity for further legal guidance is clear concerning software in general, it is not evident that AI or machine learning require a special treatment in the near future.

Given that the framework already delineates the requirements to which medical devices, software and consequently also AI have to conform, we argue that there is no evident need for legal interventions concerning those requirements specifically for AI models. We have yet to see a compelling argument that AI requires a total rethinking of the principles that are used to regulate medical devices nor that there are characteristics of AI up and beyond software that require special considerations.

While there is a potential for failure when AI models are retrained on new data and put into production unchecked, this is no different from other QA/QMS issues that can arise in software and even in traditional hardware devices. If new versions of software are pushed without validation or a new production process or material is used for a device, there is a need for a level of reassessment depending on the degree of the change and the risk that it entails. This make clear that the changes that are possible within the context of a validated product depend on the technical ability to perform the verification and validation of those changes in light of the requirements given by the applicable Regulations.

Software has a potential for faster, iterative, development and deployment. Verification is part of that process (CI & CD). However, validation remains a concern. For AI, the lack of feature engineering, or the difference between explicit features and inference leads to difficulties in guaranteeing robustness. However, the regulatory requirements are clear that this robustness needs to be validated. Common specification is an instrument that can be used to state what the state of the art of verification and validation in relation to different AI models is.

AI models are too heterogeneous to come up with an exhaustive at this time and a topic that is continuously developing at a rapid pace. However, the Commission may create a common specification,[Art 54(2)(b) jo. art 9 @REGULATIONEU2017a] which is an interesting option for regulating machine learning. While heterogeneity of the models can cause issues in creating a *common* specification, we do like to make some general suggestions for some important elements that this common specification should address.

-   The TD-certificate needs to contain detailed information on the design of a device. Applying this to AI models there should be an explanation of inner workings understandable by an expert in the field.

-   It should delineate standards for those elements in developing a model that are specific to AI. This includes the certain standard in the way that the data is prepared and handled. The standard needs to guarantee that when a dataset is split into different parts to be used for training, testing and validation that these are accidentally or [^22]

-   There should be standards for accuracy and robustness. Preferably these are verifiable by the notified bodies or other external parties.

-   Clarification on how to deal with retraining. Either by having a QMS system that can deal with the variability introduced by retraining or by treating it as a modification of the device and apply the respective provisions in the Regulation to that change.

At this time, we argue that while software, specifically AI models, may be more complex or technically challenging, but this is not something for which the regulatory route is the best approach. Instead, using medical devices that involve software such as AI requires further developments in software validations in general and validation of AI models in particular.
